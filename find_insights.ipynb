{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains the solution to the Assignment shared\n",
    "\n",
    "Author: Utkarsh Desai\n",
    "\n",
    "Only the next cell needs an update, the rest of the notebook can be executed as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required\n",
    "Provide the OpenAI API Key and the Path to the input file with feedbacks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = \"\"\n",
    "INPUT_FILE = \"spotify-reviews.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import collections\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# For this analysis, we select only a subset of the columns which are considered useful\n",
    "selected_data = data[['Source', 'CreatedAt', 'Content', 'metadata.Version', 'metadata.Score', 'metadata.Rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data has two separate fields for rating, one each for app store and play store.\n",
    "# Unify them so we only need to deal with a single field\n",
    "selected_data['metadata.Rating'] = selected_data['metadata.Rating'].fillna(0)\n",
    "selected_data['metadata.Score'] = selected_data['metadata.Score'].fillna(0)\n",
    "\n",
    "# Creata a new field and drop the older ones\n",
    "selected_data['RatingScore'] = selected_data['metadata.Rating'] + selected_data['metadata.Score']\n",
    "selected_data = selected_data.drop(columns=['metadata.Rating' ,'metadata.Score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more cleaning and processing\n",
    "selected_data['metadata.Version'] = selected_data['metadata.Version'].fillna(0)\n",
    "selected_data['CreatedAt'] = pd.to_datetime(selected_data['CreatedAt'])\n",
    "selected_data['Month_Day'] = selected_data['CreatedAt'].dt.strftime('%m_%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level analysis\n",
    "We conduct some basic analysis of the dataset to see if anything stands out. \n",
    "\n",
    "This might not always be informative, but can provide insights into the data in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = selected_data.groupby('Month_Day')['RatingScore'].agg(['mean', 'std']).reset_index()\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.errorbar(grouped['Month_Day'], grouped['mean'], yerr=grouped['std'].apply(lambda x: x**0.5), fmt='o', \n",
    "             capsize=5, linestyle='-', color='b', label='Mean Rating')\n",
    "plt.xlabel('Month-Day')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Mean Rating per Day with Variance Bars')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see there is small drop in average ratings on 02/04. Might be worthwhile to see if something happened on that day.\n",
    "\n",
    "Lets do a similar analysis on the App version instead of the feedback date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_ver = selected_data.groupby('metadata.Version')['RatingScore'].agg(['mean', 'std', np.count_nonzero]).reset_index()\n",
    "grouped_by_ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casually looking at the data, we can see all ratings for the version 9.0.12.567 are at 1-star. Also, 9.0.14.561 has a lower than usual mean rating\n",
    "\n",
    "While there are only 4 (or 17) feedbacks, analysis like this can identify if users are facing issues with a particular App version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typically, we can also do a similar, high level analysis using word frequencies, identifying terms and phrases used often via statistical methods.**\n",
    "\n",
    "**But since we are going to do a deeper analysis anyway, for now we will not do such a term frequency based analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "For what follows, we will need a sparate dataset with some manuallly annotated labels that we can use to judge the quality of model results.\n",
    "\n",
    "Typically you can create a random subset of the dataset - or from a similar distribution - and keep that for evaluation\n",
    "\n",
    "In this case, we have a small set of feedback from PlayStore and almost all feedback is from the AppStore.\n",
    "\n",
    "So, to keep things simple, we use the PlayStore data as the evaluation set and the AppStore data for the rest of the analysis. Again, this is only for this specific case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appstore_data = selected_data[selected_data['Source']=='Appstore']\n",
    "playstore_data = selected_data[selected_data['Source']=='Playstore']\n",
    "appstore_data.shape, playstore_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PlayStore data is manually annotated to indicate whether the feedback is informative and about the App, or is noisy, vague or generic feedback.\n",
    "\n",
    "The annotated file is provided and we will be using it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv(\"labeled_data.csv\", encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM based Analysis\n",
    "\n",
    "We now initialize the OpenAI client and perform deeper analysis using the LLM models (gpt-4o-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from model_wrappers import OpenAILLM, OpenAIEmbedding\n",
    "import utils\n",
    "import prompt_collection as pmpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "llm = OpenAILLM(client, model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Informative Feedback\n",
    "\n",
    "Since we do not have any annotations that help us identify which feedbacks are useful, we can use an LLM to help us here.\n",
    "\n",
    "We prompt an LLM to classfiy the feedeback for us and use the labels for the rest of the pipeline, discarding feedback that is not informative.\n",
    "\n",
    "**But before that, lets establish a baseline and see how well the prompt and the LLM are able to generate the correct labels. We use the manually annotated dataset for this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the labeled dataset, extract all feedback, join it together and prepare an input for the LLM\n",
    "joined_labeled_feedback = utils.process_batch(\n",
    "    input_feedbacks=labeled_data['Content'],\n",
    "    add_idx=True\n",
    ")\n",
    "\n",
    "# prepare a message structure for calling the LLM\n",
    "benchmark_messages = utils.prep_messages(pmpt.info_system_prompt, joined_labeled_feedback)\n",
    "\n",
    "# Call the LLM\n",
    "benchmark_response = llm.call(benchmark_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predictions from the LLM output\n",
    "benchmark_predictions = utils.clean_info_response(benchmark_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some basic metrics on how well the LLM is able to classify the data\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(labeled_data['Informative'], benchmark_predictions))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "confusion_matrix(labeled_data['Informative'], benchmark_predictions),  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have baseline performance measure for this method, we can now classify all the feedback (AppStore)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify all feedback (takes about 4 min)\n",
    "all_ratings = []\n",
    "all_content = appstore_data['Content']\n",
    "\n",
    "LLM_INPUT_BATCH_LENGTH = 100\n",
    "\n",
    "for i in range(0, len(all_content), LLM_INPUT_BATCH_LENGTH):\n",
    "    input_batch = all_content[i:i + LLM_INPUT_BATCH_LENGTH]\n",
    "    llm_input = utils.process_batch(\n",
    "        input_feedbacks=input_batch,\n",
    "        add_idx=True\n",
    "    )\n",
    "    batch_messages = utils.prep_messages(pmpt.info_system_prompt, llm_input) \n",
    "    batch_response = llm.call(batch_messages)\n",
    "    batch_predictions = utils.clean_info_response(batch_response)\n",
    "    if len(batch_predictions) != len(input_batch):\n",
    "        print(\"Mismatch in Input/Output detected\")\n",
    "    all_ratings.extend(batch_predictions)\n",
    "    print(i, len(input_batch), len(batch_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dataset with the new labels\n",
    "appstore_data['Informative'] = all_ratings\n",
    "\n",
    "# Now that we have informative feedback, we will only work with those rows\n",
    "informative_feedback = appstore_data[appstore_data['Informative']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to answer questions and extract insights based on the feedbacks available.\n",
    "\n",
    "For complaint related questions, we will only analyze poor feedback (rating < 3)\n",
    "\n",
    "One can always argue that complaints can be present in high rating feedback as well, but for this exercise we will go with this assumption to demonstrate the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_feedback = informative_feedback[informative_feedback['RatingScore']<3]\n",
    "poor_feedback_inputs = utils.process_batch(poor_feedback['Content'], add_idx=False)\n",
    "all_feedback_inputs = utils.process_batch(informative_feedback['Content'], add_idx=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_question = \"###Question: based on the provided set of user feedback, what are the top 5 bugs reported by users? \\n ###Feedback:\\n\"\n",
    "bugs_prompt = pmpt.generic_qa_prompt + bugs_question + poor_feedback_inputs\n",
    "bugs_message = utils.prep_messages(sys_prompt=None, message_content=bugs_prompt)\n",
    "bugs_response = llm.call(bugs_message)\n",
    "display(Markdown(bugs_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement_question = \"###Question: based on the provided set of user feedback, what are the top 3 suggestions from users regarding *Ads*? \\n ###Feedback:\\n\"\n",
    "improvement_prompt = pmpt.generic_qa_prompt + improvement_question + all_feedback_inputs\n",
    "improvement_message = utils.prep_messages(sys_prompt=None, message_content=improvement_prompt)\n",
    "improvement_response = llm.call(improvement_message)\n",
    "display(Markdown(improvement_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ads Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsissue_question = \"###Question: based on the provided set of user feedback, what are the top 3 issues from users regarding Ads? \\n ###Feedback:\\n\"\n",
    "adsissue_prompt = pmpt.generic_qa_prompt + adsissue_question + poor_feedback_inputs\n",
    "adsissue_message = utils.prep_messages(sys_prompt=None, message_content=adsissue_prompt)\n",
    "adsissue_response = llm.call(adsissue_message)\n",
    "display(Markdown(adsissue_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pricing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing_question =  \"###Question: based on the provided set of user feedback, what are the users talking about the pricing of the service? \\n ###Feedback:\\n\"\n",
    "pricing_prompt = pmpt.generic_qa_prompt + pricing_question + all_feedback_inputs\n",
    "pricing_message = utils.prep_messages(sys_prompt=None, message_content=pricing_prompt)\n",
    "pricing_response = llm.call(pricing_message)\n",
    "display(Markdown(pricing_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cells helped identify specific issues or answered particular questions about the feedback that users shared.\n",
    "\n",
    "We can also analyze the feedback in an alternate, complimentary fashion to reveal new insights.\n",
    "\n",
    "**We will project the feedback into an embedding space and then see if we can identify distinct clusters in this space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all informative feedback\n",
    "embed_model = OpenAIEmbedding(client)\n",
    "embeddings = embed_model.get_embeddings(informative_feedback['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to find 10 clusters\n",
    "labels = utils.cluster_documents_kmeans(embeddings, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list  = list(informative_feedback['Content'])\n",
    "clusters = collections.defaultdict(list)\n",
    "for idx, label in enumerate(labels):\n",
    "    clusters[label].append(documents_list[idx])  # Store document in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cluster(documents):\n",
    "    prompt_input_docs = utils.process_batch(documents)\n",
    "    summ_prompt = pmpt.cluster_summ_prompt + prompt_input_docs\n",
    "    summ_message = utils.prep_messages(sys_prompt=None, message_content=summ_prompt)\n",
    "    summ_response = llm.call(summ_message)\n",
    "    return summ_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster, lets now generate a 1-2 word desctiption and a 1-line summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "\n",
    "for k in clusters.keys():\n",
    "    docs = clusters[k]\n",
    "    summary = summarize_cluster(docs)\n",
    "    summaries[k] = summary\n",
    "\n",
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each cluster captures a particular theme. Although, feedback regarding Ads or Pricing seems to be prevanlent everywhere."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
